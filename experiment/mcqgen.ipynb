{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.getenv(\"MY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=api_key,temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumber=5 \\nsubject=\"data science\"\\ntone=\"simple\"\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "number=5 \n",
    "subject=\"data science\"\n",
    "tone=\"simple\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions and options are unique, not repeated, and varied in structure. Ensure that each question is distinct and tailored to different parts of the content.\n",
    "Ensure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make {number} unique MCQs.\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['number', 'response_json', 'subject', 'text', 'tone'], template='\\nText:{text}\\nYou are an expert MCQ maker. Given the above text, it is your job to create a quiz of {number} multiple choice questions for {subject} students in {tone} tone. \\nMake sure the questions and options are unique, not repeated, and varied in structure. Ensure that each question is distinct and tailored to different parts of the content.\\nEnsure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make {number} unique MCQs.\\n### RESPONSE_JSON\\n{response_json}\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_generation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\SelfContentMCQGen\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "quiz_chain=LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['number', 'response_json', 'subject', 'text', 'tone'], template='\\nText:{text}\\nYou are an expert MCQ maker. Given the above text, it is your job to create a quiz of {number} multiple choice questions for {subject} students in {tone} tone. \\nMake sure the questions and options are unique, not repeated, and varied in structure. Ensure that each question is distinct and tailored to different parts of the content.\\nEnsure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make {number} unique MCQs.\\n### RESPONSE_JSON\\n{response_json}\\n'), llm=GoogleGenerativeAI(model='models/text-bison-001', google_api_key=SecretStr('**********'), temperature=0.5, client=<module 'google.generativeai' from 'c:\\\\Users\\\\Admin\\\\SelfContentMCQGen\\\\env\\\\lib\\\\site-packages\\\\google\\\\generativeai\\\\__init__.py'>), output_key='quiz')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "    chains=[quiz_chain, review_chain], \n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    output_variables=[\"quiz\", \"review\"], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdf_reader=PyPDF2.PdfReader(r\"C:\\\\Complete_Content\\\\All_Project\\\\TEST_FOR_EVERYTHING\\\\langchain\\\\experiments\\\\Big Data Assessment.pdf\")\\ntext = \"\"\\nfor page in pdf_reader.pages:\\n    text += page.extract_text()'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"pdf_reader=PyPDF2.PdfReader(r\"C:\\Complete_Content\\All_Project\\TEST_FOR_EVERYTHING\\langchain\\experiments\\Big Data Assessment.pdf\")\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file_path=r\"C:\\\\Users\\\\Admin\\\\SelfContentMCQGen\\\\test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mcq_count=5\\ngrade=1\\ntone=\"simple'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"mcq_count=5\n",
    "grade=1\n",
    "tone=\"simple\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=5 \n",
    "SUBJECT=\"data science\"\n",
    "TONE=\"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz of 5 multiple choice questions for data science students in simple tone. \n",
      "Make sure the questions and options are unique, not repeated, and varied in structure. Ensure that each question is distinct and tailored to different parts of the content.\n",
      "Ensure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 5 unique MCQs.\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for data science students.You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
      "if the quiz is not at per with the cognitive and analytical abilities of the students,update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
      "Quiz_MCQs:\n",
      "```json\n",
      "{\n",
      "  \"1\": {\n",
      "    \"mcq\": \"What is the first step of the CRISP-DM process?\",\n",
      "    \"options\": {\n",
      "      \"a\": \"Define the problem\",\n",
      "      \"b\": \"Gather data\",\n",
      "      \"c\": \"Clean data\",\n",
      "      \"d\": \"Analyze data\"\n",
      "    },\n",
      "    \"correct\": \"a\"\n",
      "  },\n",
      "  \"2\": {\n",
      "    \"mcq\": \"Which of the following is not a type of machine learning algorithm?\",\n",
      "    \"options\": {\n",
      "      \"a\": \"Supervised learning\",\n",
      "      \"b\": \"Unsupervised learning\",\n",
      "      \"c\": \"Reinforcement learning\",\n",
      "      \"d\": \"Natural language processing\"\n",
      "    },\n",
      "    \"correct\": \"d\"\n",
      "  },\n",
      "  \"3\": {\n",
      "    \"mcq\": \"What is the most common type of data used in machine learning?\",\n",
      "    \"options\": {\n",
      "      \"a\": \"Numerical data\",\n",
      "      \"b\": \"Categorical data\",\n",
      "      \"c\": \"Text data\",\n",
      "      \"d\": \"Images\"\n",
      "    },\n",
      "    \"correct\": \"a\"\n",
      "  },\n",
      "  \"4\": {\n",
      "    \"mcq\": \"Which of the following is not a metric used to evaluate the performance of a machine learning model?\",\n",
      "    \"options\": {\n",
      "      \"a\": \"Accuracy\",\n",
      "      \"b\": \"Precision\",\n",
      "      \"c\": \"Recall\",\n",
      "      \"d\": \"Mean squared error\"\n",
      "    },\n",
      "    \"correct\": \"d\"\n",
      "  },\n",
      "  \"5\": {\n",
      "    \"mcq\": \"Which of the following is not a common challenge faced when working with machine learning data?\",\n",
      "    \"options\": {\n",
      "      \"a\": \"Bias\",\n",
      "      \"b\": \"Variance\",\n",
      "      \"c\": \"Overfitting\",\n",
      "      \"d\": \"Underfitting\"\n",
      "    },\n",
      "    \"correct\": \"d\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Check from an expert English Writer of the above quiz:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = generate_evaluate_chain.invoke(\n",
    "    {\n",
    "        \"text\": TEXT,\n",
    "        \"number\": NUMBER,\n",
    "        \"subject\": SUBJECT,\n",
    "        \"tone\": TONE,\n",
    "        \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"1\": {\\n    \"mcq\": \"What is the first step of the CRISP-DM process?\",\\n    \"options\": {\\n      \"a\": \"Define the problem\",\\n      \"b\": \"Gather data\",\\n      \"c\": \"Clean data\",\\n      \"d\": \"Analyze data\"\\n    },\\n    \"correct\": \"a\"\\n  },\\n  \"2\": {\\n    \"mcq\": \"Which of the following is not a type of machine learning algorithm?\",\\n    \"options\": {\\n      \"a\": \"Supervised learning\",\\n      \"b\": \"Unsupervised learning\",\\n      \"c\": \"Reinforcement learning\",\\n      \"d\": \"Natural language processing\"\\n    },\\n    \"correct\": \"d\"\\n  },\\n  \"3\": {\\n    \"mcq\": \"What is the most common type of data used in machine learning?\",\\n    \"options\": {\\n      \"a\": \"Numerical data\",\\n      \"b\": \"Categorical data\",\\n      \"c\": \"Text data\",\\n      \"d\": \"Images\"\\n    },\\n    \"correct\": \"a\"\\n  },\\n  \"4\": {\\n    \"mcq\": \"Which of the following is not a metric used to evaluate the performance of a machine learning model?\",\\n    \"options\": {\\n      \"a\": \"Accuracy\",\\n      \"b\": \"Precision\",\\n      \"c\": \"Recall\",\\n      \"d\": \"Mean squared error\"\\n    },\\n    \"correct\": \"d\"\\n  },\\n  \"5\": {\\n    \"mcq\": \"Which of the following is not a common challenge faced when working with machine learning data?\",\\n    \"options\": {\\n      \"a\": \"Bias\",\\n      \"b\": \"Variance\",\\n      \"c\": \"Overfitting\",\\n      \"d\": \"Underfitting\"\\n    },\\n    \"correct\": \"d\"\\n  }\\n}\\n```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_str=response.get(\"quiz\")\n",
    "quiz_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_str = quiz_str.strip(\"```json\").strip(\"```\")\n",
    "\n",
    "quiz_dict = json.loads(quiz_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'mcq': 'What is the first step of the CRISP-DM process?',\n",
       "  'options': {'a': 'Define the problem',\n",
       "   'b': 'Gather data',\n",
       "   'c': 'Clean data',\n",
       "   'd': 'Analyze data'},\n",
       "  'correct': 'a'},\n",
       " '2': {'mcq': 'Which of the following is not a type of machine learning algorithm?',\n",
       "  'options': {'a': 'Supervised learning',\n",
       "   'b': 'Unsupervised learning',\n",
       "   'c': 'Reinforcement learning',\n",
       "   'd': 'Natural language processing'},\n",
       "  'correct': 'd'},\n",
       " '3': {'mcq': 'What is the most common type of data used in machine learning?',\n",
       "  'options': {'a': 'Numerical data',\n",
       "   'b': 'Categorical data',\n",
       "   'c': 'Text data',\n",
       "   'd': 'Images'},\n",
       "  'correct': 'a'},\n",
       " '4': {'mcq': 'Which of the following is not a metric used to evaluate the performance of a machine learning model?',\n",
       "  'options': {'a': 'Accuracy',\n",
       "   'b': 'Precision',\n",
       "   'c': 'Recall',\n",
       "   'd': 'Mean squared error'},\n",
       "  'correct': 'd'},\n",
       " '5': {'mcq': 'Which of the following is not a common challenge faced when working with machine learning data?',\n",
       "  'options': {'a': 'Bias',\n",
       "   'b': 'Variance',\n",
       "   'c': 'Overfitting',\n",
       "   'd': 'Underfitting'},\n",
       "  'correct': 'd'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz_dict.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'What is the first step of the CRISP-DM process?',\n",
       "  'Choices': 'a: Define the problem | b: Gather data | c: Clean data | d: Analyze data',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'Which of the following is not a type of machine learning algorithm?',\n",
       "  'Choices': 'a: Supervised learning | b: Unsupervised learning | c: Reinforcement learning | d: Natural language processing',\n",
       "  'Correct': 'd'},\n",
       " {'MCQ': 'What is the most common type of data used in machine learning?',\n",
       "  'Choices': 'a: Numerical data | b: Categorical data | c: Text data | d: Images',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'Which of the following is not a metric used to evaluate the performance of a machine learning model?',\n",
       "  'Choices': 'a: Accuracy | b: Precision | c: Recall | d: Mean squared error',\n",
       "  'Correct': 'd'},\n",
       " {'MCQ': 'Which of the following is not a common challenge faced when working with machine learning data?',\n",
       "  'Choices': 'a: Bias | b: Variance | c: Overfitting | d: Underfitting',\n",
       "  'Correct': 'd'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Data_Science_Quiz.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
